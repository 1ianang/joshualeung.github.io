<!DOCTYPE html>
<html lang="en">
  <head>
    

    
<script>!function(){var e=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,t=localStorage.getItem("use-color-scheme")||"auto";("dark"===t||e&&"light"!==t)&&document.documentElement.classList.toggle("dark",!0)}()</script>
    

<meta charset="utf-8" >

<title>RAG中的多向量检索实践</title>
<meta name="keywords" content="RAG中的多向量检索实践, 击壤而歌">
<meta name="description" content="在之前的文章中， 我们介绍了RAG中常见的文档分块策略。 这些分块策略对于文档中的任何信息都只有一个与之对应的向量(文档重叠的部分除外)。 在实际应用场景中， 我们可以通过为每一个文档块关联多个向量来进一步提升检索的效果。 
创建多向量有多">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="RAG中的多向量检索实践">
<meta property="og:description" content="在之前的文章中， 我们介绍了RAG中常见的文档分块策略。 这些分块策略对于文档中的任何信息都只有一个与之对应的向量(文档重叠的部分除外)。 在实际应用场景中， 我们可以通过为每一个文档块关联多个向量来进一步提升检索的效果。 
创建多向量有多">

<link rel="shortcut icon" href="/favicon.ico">
<link rel="stylesheet" href="/style/main.css">

  <!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><link rel="stylesheet" href="/style/simple-lightbox.min.css"><meta name="generator" content="Hexo 7.3.0"></head>
  <body>
    <div id="app" class="main">

<div class="site-header-container">
  <div class="site-header">
    <div class="left">
      <a href="http://orrrrz.github.io">
        <img class="avatar" src="/images/avatar.png" alt="logo" width="32px" height="32px">
      </a>
      <a href="http://orrrrz.github.io">
        <h1 class="site-title">击壤而歌</h1>
      </a>
    </div>
    <div class="right">
        <i class="icon menu-switch icon-menu-outline" ></i>
    </div>
  </div>
</div>

<div class="menu-container" style="height: 0;opacity: 0;">
<nav class="menu-list">
  
    
      <a href="/" class="menu purple-link">
        首页
      </a>
    
  
    
      <a href="/tags" class="menu purple-link">
        标签
      </a>
    
  
    
      <a href="/archives" class="menu purple-link">
        归档
      </a>
    
  
    
      <a href="/about" class="menu purple-link">
        关于
      </a>
    
  
</nav>
</div>



  <div class="content-container">
    <div class="post-detail">
      
      <h2 class="post-title">RAG中的多向量检索实践</h2>
      <div class="post-info post-detail-info">
        <span><i class="icon icon-calendar-outline"></i> 2025-01-18</span>
        
          <span>
          <i class="icon icon-pricetags-outline"></i>
            
              <a href="/tags/RAG/">
              RAG
                
                  ，
                
              </a>
            
              <a href="/tags/Retrieval-Augmented-Generation/">
              Retrieval Augmented Generation
                
                  ，
                
              </a>
            
              <a href="/tags/Multi-Vector/">
              Multi-Vector
                
              </a>
            
          </span>
        
      </div>
      <div class="post-content-wrapper">
        <div class="post-content">
          <p>在<a href="https://orrrrz.github.io/2025/01/17/rag/chunking/">之前的文章</a>中， 我们介绍了RAG中常见的文档分块策略。 这些分块策略对于文档中的任何信息都只有一个与之对应的向量(文档重叠的部分除外)。 在实际应用场景中， 我们可以通过为每一个文档块关联多个向量来进一步提升检索的效果。 </p>
<p>创建多向量有多种方式， 包括:</p>
<ul>
<li>为每一个文档块创建子分块， 基于子分块和原始文档块进行匹配， 返回原始文档块。</li>
<li>为每一个文档块创建摘要(Summary)， 基于摘要和原始文档块进行匹配， 返回原始文档块。</li>
<li>为每一个文档块创建假设查询(Hypothetical Questions)， 基于假设查询和原始文档块进行匹配， 返回原始文档块。</li>
</ul>
<p>本文通过代码示例来说明如何在 LangChain中使用多向量检索。</p>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>为了在使用多向量查询时， 能够返回原始文档的原文, 需要将向量索引与原始文档块分开存储, 向量索引存储称为 vector sctore, 而原始文档块存储则称为 doc store。</p>
<p>以下代码基于 Chroma 创建了向量存储， 同时通过 InMemoryByteStore 创建了内存中的原始文档块存储。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.storage <span class="keyword">import</span> InMemoryByteStore</span><br><span class="line"><span class="keyword">from</span> langchain_chroma <span class="keyword">import</span> Chroma</span><br><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> TextLoader</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> RecursiveCharacterTextSplitter</span><br><span class="line"></span><br><span class="line">loaders = [</span><br><span class="line">    TextLoader(<span class="string">&quot;paul_graham_essay.txt&quot;</span>),</span><br><span class="line">    TextLoader(<span class="string">&quot;state_of_the_union.txt&quot;</span>),</span><br><span class="line">]</span><br><span class="line">docs = []</span><br><span class="line"><span class="keyword">for</span> loader <span class="keyword">in</span> loaders:</span><br><span class="line">    docs.extend(loader.load())</span><br><span class="line">text_splitter = RecursiveCharacterTextSplitter(chunk_size=<span class="number">10000</span>)</span><br><span class="line">docs = text_splitter.split_documents(docs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The vectorstore to use to index the child chunks</span></span><br><span class="line">vectorstore = Chroma(</span><br><span class="line">    collection_name=<span class="string">&quot;full_documents&quot;</span>, embedding_function=OpenAIEmbeddings()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> uuid</span><br><span class="line"><span class="keyword">from</span> langchain.retrievers.multi_vector <span class="keyword">import</span> MultiVectorRetriever</span><br><span class="line"></span><br><span class="line">store = InMemoryByteStore()</span><br><span class="line">id_key = <span class="string">&quot;doc_id&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The retriever (empty to start)</span></span><br><span class="line">retriever = MultiVectorRetriever(</span><br><span class="line">    vectorstore=vectorstore,</span><br><span class="line">    byte_store=store,</span><br><span class="line">    id_key=id_key,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">doc_ids = [<span class="built_in">str</span>(uuid.uuid4()) <span class="keyword">for</span> _ <span class="keyword">in</span> docs]</span><br></pre></td></tr></table></figure>

<p>对于一个文档（这里是指初始分块之后的文档）， 有多种创建多向量的方式。 </p>
<h2 id="子分块向量"><a href="#子分块向量" class="headerlink" title="子分块向量"></a>子分块向量</h2><p>在初始文档块的基础上， 将每个文档块划分为多个子分块, 同时将子分块与原始分块进行关联， 然后加入向量和文档索引。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The splitter to use to create smaller chunks</span></span><br><span class="line">child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=<span class="number">400</span>)</span><br><span class="line"></span><br><span class="line">sub_docs = []</span><br><span class="line"><span class="keyword">for</span> i, doc <span class="keyword">in</span> <span class="built_in">enumerate</span>(docs):</span><br><span class="line">    _<span class="built_in">id</span> = doc_ids[i]</span><br><span class="line">    _sub_docs = child_text_splitter.split_documents([doc])</span><br><span class="line">    <span class="keyword">for</span> _doc <span class="keyword">in</span> _sub_docs:</span><br><span class="line">        _doc.metadata[id_key] = _<span class="built_in">id</span></span><br><span class="line">    sub_docs.extend(_sub_docs)</span><br><span class="line">retriever.vectorstore.add_documents(sub_docs)</span><br><span class="line">retriever.docstore.mset(<span class="built_in">list</span>(<span class="built_in">zip</span>(doc_ids, docs)))</span><br></pre></td></tr></table></figure>
<p>如果通过向量索引来检索， 返回的是子文档块:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">retriever.vectorstore.similarity_search(<span class="string">&quot;justice breyer&quot;</span>)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>输出:</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Document(page_content=&#x27;Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.&#x27;, metadata=&#123;&#x27;doc_id&#x27;: &#x27;064eca46-a4c4-4789-8e3b-583f9597e54f&#x27;, &#x27;source&#x27;: &#x27;state_of_the_union.txt&#x27;&#125;)</span><br></pre></td></tr></table></figure>
<p>如果通过多向量 retriever 来检索， 同样是通过向量索引来匹配， 但返回的是原始文档块:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">len</span>(retriever.invoke(<span class="string">&quot;justice breyer&quot;</span>)[<span class="number">0</span>].page_content)</span><br></pre></td></tr></table></figure>
<p>输出:</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">9875</span><br></pre></td></tr></table></figure>

<p>在LangChain中， 向量检索时默认使用的排序指标是余弦相似度， 也可以使用其他指标，如结合了相似性与多样性的MMR (Maximal Marginal Relevance):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.retrievers.multi_vector <span class="keyword">import</span> SearchType</span><br><span class="line"></span><br><span class="line">retriever.search_type = SearchType.mmr</span><br><span class="line"><span class="built_in">len</span>(retriever.invoke(<span class="string">&quot;justice breyer&quot;</span>)[<span class="number">0</span>].page_content)</span><br></pre></td></tr></table></figure>
<p>输出:</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">9875</span><br></pre></td></tr></table></figure>

<h2 id="文档摘要"><a href="#文档摘要" class="headerlink" title="文档摘要"></a>文档摘要</h2><p>摘要(Summary) 是文档块核心语义信息的浓缩。通过LLM对文档块进行总结， 有助于编码模型更准确地表征语义。 </p>
<p>以下代码展示了如何通过LLM对文档块进行总结， 得到文档块的摘要:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> getpass</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">import</span> uuid</span><br><span class="line"><span class="keyword">from</span> langchain_core.documents <span class="keyword">import</span> Document</span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.environ.get(<span class="string">&quot;OPENAI_API_KEY&quot;</span>):</span><br><span class="line">  os.environ[<span class="string">&quot;OPENAI_API_KEY&quot;</span>] = getpass.getpass(<span class="string">&quot;Enter API key for OpenAI: &quot;</span>)</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(model=<span class="string">&quot;gpt-4o-mini&quot;</span>)</span><br><span class="line"></span><br><span class="line">chain = (</span><br><span class="line">    &#123;<span class="string">&quot;doc&quot;</span>: <span class="keyword">lambda</span> x: x.page_content&#125;</span><br><span class="line">    | ChatPromptTemplate.from_template(<span class="string">&quot;Summarize the following document:\n\n&#123;doc&#125;&quot;</span>)</span><br><span class="line">    | llm</span><br><span class="line">    | StrOutputParser()</span><br><span class="line">)</span><br><span class="line">summaries = chain.batch(docs, &#123;<span class="string">&quot;max_concurrency&quot;</span>: <span class="number">5</span>&#125;)</span><br></pre></td></tr></table></figure>
<p>对摘要进行向量编码和索引: </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">vectorstore = Chroma(collection_name=<span class="string">&quot;summaries&quot;</span>, embedding_function=OpenAIEmbeddings())</span><br><span class="line"></span><br><span class="line">store = InMemoryByteStore()</span><br><span class="line">id_key = <span class="string">&quot;doc_id&quot;</span></span><br><span class="line"><span class="comment"># The retriever (empty to start)</span></span><br><span class="line">retriever = MultiVectorRetriever(</span><br><span class="line">    vectorstore=vectorstore,</span><br><span class="line">    byte_store=store,</span><br><span class="line">    id_key=id_key,</span><br><span class="line">)</span><br><span class="line">doc_ids = [<span class="built_in">str</span>(uuid.uuid4()) <span class="keyword">for</span> _ <span class="keyword">in</span> docs]</span><br><span class="line"></span><br><span class="line">summary_docs = [</span><br><span class="line">    Document(page_content=s, metadata=&#123;id_key: doc_ids[i]&#125;)</span><br><span class="line">    <span class="keyword">for</span> i, s <span class="keyword">in</span> <span class="built_in">enumerate</span>(summaries)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">retriever.vectorstore.add_documents(summary_docs)</span><br><span class="line">retriever.docstore.mset(<span class="built_in">list</span>(<span class="built_in">zip</span>(doc_ids, docs)))</span><br></pre></td></tr></table></figure>

<h2 id="假设查询"><a href="#假设查询" class="headerlink" title="假设查询"></a>假设查询</h2><p>与HyDE(Hypothetical Document, 见论文 Precise Zero-Shot Dense Retrieval without Relevance Labels)通过构造假设文档的思路相对， 这里通过文档来构造假设查询。 检索时， 在查询向量与假设查询向量之间进行匹配， 返回原文档块。 </p>
<p>通过LLM构造假设查询:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span></span><br><span class="line"><span class="keyword">from</span> pydantic <span class="keyword">import</span> BaseModel, Field</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">HypotheticalQuestions</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Generate hypothetical questions.&quot;&quot;&quot;</span></span><br><span class="line">    questions: <span class="type">List</span>[<span class="built_in">str</span>] = Field(..., description=<span class="string">&quot;List of questions&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">chain = (</span><br><span class="line">    &#123;<span class="string">&quot;doc&quot;</span>: <span class="keyword">lambda</span> x: x.page_content&#125;</span><br><span class="line">    <span class="comment"># Only asking for 3 hypothetical questions, but this could be adjusted</span></span><br><span class="line">    | ChatPromptTemplate.from_template(</span><br><span class="line">        <span class="string">&quot;Generate a list of exactly 3 hypothetical questions that the below document could be used to answer:\n\n&#123;doc&#125;&quot;</span></span><br><span class="line">    )</span><br><span class="line">    | ChatOpenAI(max_retries=<span class="number">0</span>, model=<span class="string">&quot;gpt-4o&quot;</span>).with_structured_output(</span><br><span class="line">        HypotheticalQuestions</span><br><span class="line">    )</span><br><span class="line">    | (<span class="keyword">lambda</span> x: x.questions)</span><br><span class="line">)</span><br><span class="line">hypothetical_questions = chain.batch(docs, &#123;<span class="string">&quot;max_concurrency&quot;</span>: <span class="number">5</span>&#125;)</span><br></pre></td></tr></table></figure>

<p>对假设查询进行向量编码和索引:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The vectorstore to use to index the child chunks</span></span><br><span class="line">vectorstore = Chroma(</span><br><span class="line">    collection_name=<span class="string">&quot;hypo-questions&quot;</span>, embedding_function=OpenAIEmbeddings()</span><br><span class="line">)</span><br><span class="line"><span class="comment"># The storage layer for the parent documents</span></span><br><span class="line">store = InMemoryByteStore()</span><br><span class="line">id_key = <span class="string">&quot;doc_id&quot;</span></span><br><span class="line"><span class="comment"># The retriever (empty to start)</span></span><br><span class="line">retriever = MultiVectorRetriever(</span><br><span class="line">    vectorstore=vectorstore,</span><br><span class="line">    byte_store=store,</span><br><span class="line">    id_key=id_key,</span><br><span class="line">)</span><br><span class="line">doc_ids = [<span class="built_in">str</span>(uuid.uuid4()) <span class="keyword">for</span> _ <span class="keyword">in</span> docs]</span><br><span class="line"></span><br><span class="line">question_docs = []</span><br><span class="line"><span class="keyword">for</span> i, question_list <span class="keyword">in</span> <span class="built_in">enumerate</span>(hypothetical_questions):</span><br><span class="line">    question_docs.extend(</span><br><span class="line">        [Document(page_content=s, metadata=&#123;id_key: doc_ids[i]&#125;) <span class="keyword">for</span> s <span class="keyword">in</span> question_list]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">retriever.vectorstore.add_documents(question_docs)</span><br><span class="line">retriever.docstore.mset(<span class="built_in">list</span>(<span class="built_in">zip</span>(doc_ids, docs)))</span><br></pre></td></tr></table></figure>


<h2 id="多向量在多模态RAG中的应用"><a href="#多向量在多模态RAG中的应用" class="headerlink" title="多向量在多模态RAG中的应用"></a>多向量在多模态RAG中的应用</h2><p>上面介绍了多向量在文本模态上创建多向量的三种方法。 多向量只是一种将检索表征与合成表征解耦的索引范式， 对于多模态内容来说， 这也同样适用。 </p>
<p><a class="simple-lightbox" target="_blank" rel="noopener" href="https://blog.langchain.dev/content/images/2023/10/mvr_overview.png"><img   src="/images/loading.svg" data-src="https://blog.langchain.dev/content/images/2023/10/mvr_overview.png"  lazyload></a></p>
<p>在文章 <a target="_blank" rel="noopener" href="https://blog.langchain.dev/semi-structured-multi-modal-rag/">Multi-Vector Retriever for RAG on tables, text, and images</a>中， LangChain 团队介绍了将多向量应用于多模态内容的几种方式: </p>
<ul>
<li><p>使用多模态向量模型(如CLIP)来同时对图像和文本进行向量化， 通过多模态向量来检索， 但返回原始图像。 。</p>
</li>
<li><p>利用多模态LLM(如GPT4-V、LLaVA或FUYU-8b)来为图像生成文本摘要并Embedding。基于查询和摘要的相似度来检索，同时返回摘要给LLM。</p>
</li>
<li><p>利用多模态LLM(如GPT4-V、LLaVA或FUYU-8b)来为图像生成文本摘要并Embedding, 基于查询和摘要的相似度来检索， 但返回原始图片。</p>
</li>
</ul>
<p>具体使用哪种方式取决于具体的应用场景， 以及对成本、性能、检索质量的综合考量。 </p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/how_to/multi_vector/">How to retrieve using multiple vectors per document</a></li>
</ul>

        </div>
          
        <div class="top-div">
          <ol class="top-box"><li class="top-box-item top-box-level-2"><a class="top-box-link" href="#%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C"><span class="top-box-text">准备工作</span></a></li><li class="top-box-item top-box-level-2"><a class="top-box-link" href="#%E5%AD%90%E5%88%86%E5%9D%97%E5%90%91%E9%87%8F"><span class="top-box-text">子分块向量</span></a></li><li class="top-box-item top-box-level-2"><a class="top-box-link" href="#%E6%96%87%E6%A1%A3%E6%91%98%E8%A6%81"><span class="top-box-text">文档摘要</span></a></li><li class="top-box-item top-box-level-2"><a class="top-box-link" href="#%E5%81%87%E8%AE%BE%E6%9F%A5%E8%AF%A2"><span class="top-box-text">假设查询</span></a></li><li class="top-box-item top-box-level-2"><a class="top-box-link" href="#%E5%A4%9A%E5%90%91%E9%87%8F%E5%9C%A8%E5%A4%9A%E6%A8%A1%E6%80%81RAG%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8"><span class="top-box-text">多向量在多模态RAG中的应用</span></a></li><li class="top-box-item top-box-level-2"><a class="top-box-link" href="#%E5%8F%82%E8%80%83"><span class="top-box-text">参考</span></a></li></ol>
        </div>
          
      </div>
    </div>

    
      <div class="next-post">
        <a class="purple-link" href="/2025/01/17/rag/chunking/">
          <h3 class="post-title">
            下一篇：RAG中的文档分块策略
          </h3>
        </a>
      </div>
    
  </div>


    <div id="gitalk-container"></div>



    <link rel="stylesheet" href="/style/gitalk.min.css"/>
    <script src="/js/gitalk.min.js"></script>
    <script>
        const config = {"clientId":"Ov23ctHktH8663nWo7p3","clientSecret":"17ed39dfe9fd4213784f3600cbb3591c2a2d3180","repository":"orrrrz.github.io","owner":"orrrrz","createIssueManually":false};
        const gitalk = new Gitalk({
            clientID: config.clientId,
            clientSecret: config.clientSecret,
            repo: config.repository,
            owner: config.owner,
            admin: [config.owner],
            id: location.pathname.slice(1, location.pathname.lastIndexOf('/')).substring(0, 49),       // Ensure uniqueness and length less than 50
            distractionFreeMode: false,  // Facebook-like distraction free mode
            createIssueManually: config.createIssueManually ? true : false
        });

        gitalk.render('gitalk-container')

    </script>







<footer>
<div class="site-footer">
  <div class="social-container">
    
      
        <a aria-label="跳转至github" href="https://github.com/orrrrz" target="_blank">
          <i class="icon icon-github"></i>
        </a>
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  </div>
  
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> <a href="https://github.com/f-dong/hexo-theme-minimalism" target="_blank">Theme</a>
  
  
  
  
  
  
</div>
</footer>


      </div>
    </div>
    
<script id="hexo-configurations"> window.theme_config = {"image":{"lazyload_enable":true,"photo_zoom":"simple-lightbox"}}; window.is_post = true; </script>

<script src="/js/main.js"></script>


    <script async src="https://www.googletagmanager.com/gtag/js?id=G-BD6SR7X2TB"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-BD6SR7X2TB');
    </script>





  <script src="/js/simple-lightbox.min.js"></script><script>document.addEventListener('DOMContentLoaded', function() {new SimpleLightbox('.post-detail .simple-lightbox', {fileExt: false,captionsData:'alt'});});</script></body>
</html>

