<!DOCTYPE html>
<html lang="en">
  <head>
    

    
<script>!function(){var e=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,t=localStorage.getItem("use-color-scheme")||"auto";("dark"===t||e&&"light"!==t)&&document.documentElement.classList.toggle("dark",!0)}()</script>
    

<meta charset="utf-8" >

<title>大语言模型推理加速之 KV Cache</title>
<meta name="keywords" content="大语言模型推理加速之 KV Cache, 击壤而歌">
<meta name="description" content="大模型推理过程大模型的推理可以分为两个阶段:

Prefill: 基于全部Prompt进行前向推理， 得到第一个token。计算密集。
Decoding: 通过自回归方式， 递归输出新的token。内存密集。

由于 Prefill与Dec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="大语言模型推理加速之 KV Cache">
<meta property="og:description" content="大模型推理过程大模型的推理可以分为两个阶段:

Prefill: 基于全部Prompt进行前向推理， 得到第一个token。计算密集。
Decoding: 通过自回归方式， 递归输出新的token。内存密集。

由于 Prefill与Dec">

<link rel="shortcut icon" href="/favicon.ico">
<link rel="stylesheet" href="/style/main.css">

  <!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><link rel="stylesheet" href="/style/simple-lightbox.min.css"><meta name="generator" content="Hexo 7.3.0"></head>
  <body>
    <div id="app" class="main">

<div class="site-header-container">
  <div class="site-header">
    <div class="left">
      <a href="http://orrrrz.github.io">
        <img class="avatar" src="/images/avatar.png" alt="logo" width="32px" height="32px">
      </a>
      <a href="http://orrrrz.github.io">
        <h1 class="site-title">击壤而歌</h1>
      </a>
    </div>
    <div class="right">
        <i class="icon menu-switch icon-menu-outline" ></i>
    </div>
  </div>
</div>

<div class="menu-container" style="height: 0;opacity: 0;">
<nav class="menu-list">
  
    
      <a href="/" class="menu purple-link">
        首页
      </a>
    
  
    
      <a href="/tags" class="menu purple-link">
        标签
      </a>
    
  
    
      <a href="/archives" class="menu purple-link">
        归档
      </a>
    
  
    
      <a href="/about" class="menu purple-link">
        关于
      </a>
    
  
</nav>
</div>



  <div class="content-container">
    <div class="post-detail">
      
      <h2 class="post-title">大语言模型推理加速之 KV Cache</h2>
      <div class="post-info post-detail-info">
        <span><i class="icon icon-calendar-outline"></i> 2025-02-09</span>
        
          <span>
          <i class="icon icon-pricetags-outline"></i>
            
              <a href="/tags/LLM/">
              LLM
                
                  ，
                
              </a>
            
              <a href="/tags/KVCache/">
              KVCache
                
              </a>
            
          </span>
        
      </div>
      <div class="post-content-wrapper">
        <div class="post-content">
          <h1 id="大模型推理过程"><a href="#大模型推理过程" class="headerlink" title="大模型推理过程"></a>大模型推理过程</h1><p>大模型的推理可以分为两个阶段:</p>
<ul>
<li>Prefill: 基于全部Prompt进行前向推理， 得到第一个token。计算密集。</li>
<li>Decoding: 通过自回归方式， 递归输出新的token。内存密集。</li>
</ul>
<p>由于 Prefill与Decoding对资源需求的差异， 一般对这两阶段分别进行优化。 KV Cache是针对Decoding阶段的优化， 有助于提升 TPOT（Time Per Output Token）, 而对 TTFT（Time To First Token）无明显益处。</p>
<p><a class="simple-lightbox" target="_blank" rel="noopener" href="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sexO6adGhaKr7aH0.gif"><img   src="/images/loading.svg" data-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sexO6adGhaKr7aH0.gif"  lazyload></a></p>
<h1 id="KV-Cache的原理"><a href="#KV-Cache的原理" class="headerlink" title="KV Cache的原理"></a>KV Cache的原理</h1><p>KV Caching是在推理Decoding阶段中的加速策略。 它基于自回归过程中对Attention计算过程中冗余性的观察。 </p>
<p>由于 Decoder 是逐 Token进行输出的， 当前 Token的输出依赖于上一次 Token的输出。 而在每次生成新的 token时， 上一次得到的 K, V可以被重复利用。</p>
<p><a class="simple-lightbox" target="_blank" rel="noopener" href="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8xqD4AYTwn6mQXNw0uhDCg.gif"><img   src="/images/loading.svg" data-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8xqD4AYTwn6mQXNw0uhDCg.gif"  alt="Step-by-step visualization of the scaled dot-product attention in the decoder. emb_size means embedding size. Image created by the author." lazyload></a></p>
<p>因此 ， 可以将之前计算的K, V矩阵进行缓存， 仅需要在当前步骤中，计算当前 token对应的K, V矩阵。 </p>
<p><a class="simple-lightbox" target="_blank" rel="noopener" href="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uyuyOW1VBqmF5Gtv225XHQ.gif"><img   src="/images/loading.svg" data-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uyuyOW1VBqmF5Gtv225XHQ.gif"  alt="Comparison of scaled dot-product attention with and without KV caching. emb_size means embedding size. Image created by the author." lazyload></a></p>
<h1 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line">checkpoint = <span class="string">&quot;distilgpt2&quot;</span></span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(checkpoint)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> use_cache <span class="keyword">in</span> (<span class="literal">True</span>, <span class="literal">False</span>):</span><br><span class="line">    times = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):  <span class="comment"># measuring 10 generations</span></span><br><span class="line">        start = time.time()</span><br><span class="line">        model.generate(**tokenizer(<span class="string">&quot;What is KV caching?&quot;</span>, return_tensors=<span class="string">&quot;pt&quot;</span>),</span><br><span class="line">                   use_cache=use_cache,</span><br><span class="line">                   max_new_tokens=<span class="number">20</span>)</span><br><span class="line">        times.append(time.time() - start)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;TPOT <span class="subst">&#123;<span class="string">&#x27;with&#x27;</span> <span class="keyword">if</span> use_cache <span class="keyword">else</span> <span class="string">&#x27;without&#x27;</span>&#125;</span> KV caching: &quot;</span></span><br><span class="line">          <span class="string">f&quot;<span class="subst">&#123;<span class="built_in">round</span>(np.mean(times), <span class="number">3</span>)&#125;</span> +- <span class="subst">&#123;<span class="built_in">round</span>(np.std(times), <span class="number">3</span>)&#125;</span> seconds&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> use_cache <span class="keyword">in</span> (<span class="literal">True</span>, <span class="literal">False</span>):</span><br><span class="line">    times = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):  <span class="comment"># measuring 10 generations</span></span><br><span class="line">        start = time.time()</span><br><span class="line">        model.generate(**tokenizer(<span class="string">&quot;What is KV caching?&quot;</span>, return_tensors=<span class="string">&quot;pt&quot;</span>),</span><br><span class="line">                       use_cache=use_cache,</span><br><span class="line">                       max_new_tokens=<span class="number">1</span>)</span><br><span class="line">        times.append(time.time() - start)</span><br><span class="line">    <span class="built_in">print</span>(</span><br><span class="line">        <span class="string">f&quot;TTFT <span class="subst">&#123;<span class="string">&#x27;with&#x27;</span> <span class="keyword">if</span> use_cache <span class="keyword">else</span> <span class="string">&#x27;without&#x27;</span>&#125;</span> KV caching: &quot;</span></span><br><span class="line">        <span class="string">f&quot;<span class="subst">&#123;<span class="built_in">round</span>(np.mean(times), <span class="number">3</span>)&#125;</span> +- <span class="subst">&#123;<span class="built_in">round</span>(np.std(times), <span class="number">3</span>)&#125;</span> seconds&quot;</span>)</span><br></pre></td></tr></table></figure>

<h1 id="KV-Cache的代价"><a href="#KV-Cache的代价" class="headerlink" title="KV Cache的代价"></a>KV Cache的代价</h1><ul>
<li>GPU VRAM的占用<ul>
<li>由于需要在内存中缓存K, V 矩阵， 导致VRAM被大量占用。这限制了最大序列长度和batch size。</li>
<li>GPU会花费更多的时间在从缓存中加载数据上， 这限制了GPU的利用率。</li>
</ul>
</li>
<li>很多大模型推理框架都从减少KV Cache的大小&#x2F;降低KV Cache的碎片化&#x2F;提升KV Cache的访存效率等角度出发来做系统的优化。</li>
</ul>
<h1 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h1><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]  <a target="_blank" rel="noopener" href="https://medium.com/@joaolages/kv-caching-explained-276520203249">Transformers KV Caching Explained</a><br>[2]  <a target="_blank" rel="noopener" href="https://huggingface.co/blog/kv-cache-quantization">Unlocking Longer Generation with Key-Value Cache Quantization</a><br>[3]  <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/fDuFSsAhf-wXmkP1d1Ri_g"># 大模型推理加速与KV Cache（一）：什么是KV Cache</a></p>

        </div>
          
        <div class="top-div">
          <ol class="top-box"><li class="top-box-item top-box-level-1"><a class="top-box-link" href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E8%BF%87%E7%A8%8B"><span class="top-box-text">大模型推理过程</span></a></li><li class="top-box-item top-box-level-1"><a class="top-box-link" href="#KV-Cache%E7%9A%84%E5%8E%9F%E7%90%86"><span class="top-box-text">KV Cache的原理</span></a></li><li class="top-box-item top-box-level-1"><a class="top-box-link" href="#%E6%80%A7%E8%83%BD"><span class="top-box-text">性能</span></a></li><li class="top-box-item top-box-level-1"><a class="top-box-link" href="#KV-Cache%E7%9A%84%E4%BB%A3%E4%BB%B7"><span class="top-box-text">KV Cache的代价</span></a></li><li class="top-box-item top-box-level-1"><a class="top-box-link" href="#%E6%89%A9%E5%B1%95"><span class="top-box-text">扩展</span></a></li><li class="top-box-item top-box-level-1"><a class="top-box-link" href="#%E5%8F%82%E8%80%83"><span class="top-box-text">参考</span></a></li></ol>
        </div>
          
      </div>
    </div>

    
      <div class="next-post">
        <a class="purple-link" href="/2025/02/07/llm/dpo/">
          <h3 class="post-title">
            下一篇：大语言模型原理之 DPO 算法
          </h3>
        </a>
      </div>
    
  </div>


    <div id="gitalk-container"></div>



    <link rel="stylesheet" href="/style/gitalk.min.css"/>
    <script src="/js/gitalk.min.js"></script>
    <script>
        const config = {"clientId":"Ov23ctHktH8663nWo7p3","clientSecret":"17ed39dfe9fd4213784f3600cbb3591c2a2d3180","repository":"orrrrz.github.io","owner":"orrrrz","createIssueManually":false};
        const gitalk = new Gitalk({
            clientID: config.clientId,
            clientSecret: config.clientSecret,
            repo: config.repository,
            owner: config.owner,
            admin: [config.owner],
            id: location.pathname.slice(1, location.pathname.lastIndexOf('/')).substring(0, 49),       // Ensure uniqueness and length less than 50
            distractionFreeMode: false,  // Facebook-like distraction free mode
            createIssueManually: config.createIssueManually ? true : false
        });

        gitalk.render('gitalk-container')

    </script>







<footer>
<div class="site-footer">
  <div class="social-container">
    
      
        <a aria-label="跳转至github" href="https://github.com/orrrrz" target="_blank">
          <i class="icon icon-github"></i>
        </a>
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  </div>
  
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> <a href="https://github.com/f-dong/hexo-theme-minimalism" target="_blank">Theme</a>
  
  
  
  
  
  
</div>
</footer>


      </div>
    </div>
    
<script id="hexo-configurations"> window.theme_config = {"image":{"lazyload_enable":true,"photo_zoom":"simple-lightbox"}}; window.is_post = true; </script>

<script src="/js/main.js"></script>


    <script async src="https://www.googletagmanager.com/gtag/js?id=G-BD6SR7X2TB"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-BD6SR7X2TB');
    </script>





  <script src="/js/simple-lightbox.min.js"></script><script>document.addEventListener('DOMContentLoaded', function() {new SimpleLightbox('.post-detail .simple-lightbox', {fileExt: false,captionsData:'alt'});});</script></body>
</html>

